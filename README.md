# ğŸ§  STEDI Human Balance Analytics â€“ Local PySpark Project

This repository implements a local version of the **STEDI Human Balance Analytics** project using **PySpark** and **Jupyter Notebooks**, simulating the data curation process originally designed for AWS Glue and Athena.

## ğŸ“˜ Project Summary

STEDI has created a **Step Trainer** device and mobile app that help users improve their physical balance. The devices capture motion sensor data, which can be used to train a machine learning model that detects balance-improving movements. However, only customers who have provided **explicit consent** to share their data can be included in the model training pipeline.

This project focuses on building an **ETL pipeline** that:
- Loads raw sensor and customer data (Landing Zone)
- Filters and cleans the data based on consent (Trusted Zone)
- Aggregates relevant features to prepare for ML model input (Curated Zone)

---

## ğŸ§± Project Architecture

The project simulates three common data lake zones:

```
                Raw JSON Files (Landing Zone)
                      |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                            â”‚
Customer Data            Sensor Data (Accelerometer + Step Trainer)
        â”‚                            â”‚
        â””â”€â”€â”€â”€â”€â–º Trusted Zone (filter: shareWithResearchAsOfDate not null)
                                 |
                      Inner Join on email or serialNumber
                                 |
                      â–¼
             Curated Zone (machine_learning_curated)
```

---

## ğŸ“ Project Structure

```
stedi-etl-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customer/              # Raw customer JSON files
â”‚   â”œâ”€â”€ accelerometer/         # Raw accelerometer JSON files
â”‚   â””â”€â”€ step_trainer/          # Raw step trainer JSON files
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ customer_trusted.ipynb     # Create customer_trusted table
â”‚   â”œâ”€â”€ accelerometer_trusted.ipynb # Filter accelerometer data
â”‚   â”œâ”€â”€ customer_curated.ipynb     # Join customer + accelerometer
â”‚   â”œâ”€â”€ step_trainer_trusted.ipynb # Filter step trainer by curated
â”‚   â””â”€â”€ machine_learning_curated.ipynb # Final join
â”œâ”€â”€ output/
â”‚   â””â”€â”€ customer_trusted/      # Parquet files (simulating S3)
â”œâ”€â”€ README.md
```

---

## ğŸ§ª Technologies Used

| Tool             | Purpose                                 |
|------------------|-----------------------------------------|
| Python           | Primary language                        |
| PySpark          | Distributed data processing engine      |
| Jupyter Notebook | Interactive development and debugging   |
| Parquet          | Output format for local "S3 simulation" |

---

## âš™ï¸ Setup Instructions

### 1. Install Python packages

```bash
pip install pyspark jupyterlab
```

### 2. Install Java JDK (required for PySpark)

- Recommended: Java 8 or Java 11
- Download from: https://adoptium.net/en-GB/temurin/releases/

### 3. Run Jupyter Lab

```bash
cd stedi-etl-project
jupyter lab
```

Open and run the notebooks in order, starting with:
```
notebooks/customer_trusted.ipynb
```

---

## ğŸ§© ETL Workflow

### âœ… Step 1: Customer Trusted
- Load raw JSON
- Filter where `shareWithResearchAsOfDate` is not null
- Save as `customer_trusted` (Parquet)

### âœ… Step 2: Accelerometer Trusted
- Load accelerometer JSON
- Join with `customer_trusted` on email/user
- Save as `accelerometer_trusted`

### âœ… Step 3: Customer Curated
- Inner join `customer_trusted` and `accelerometer_trusted`
- Save as `customer_curated`

### âœ… Step 4: Step Trainer Trusted
- Load step trainer data
- Filter records based on `customer_curated` serial numbers
- Save as `step_trainer_trusted`

### âœ… Step 5: Machine Learning Curated
- Join `accelerometer_trusted` with `step_trainer_trusted` on timestamp
- Save as `machine_learning_curated`

---

## ğŸ“¸ Screenshots & Results

Each stage outputs Parquet files into the `output/` folder.

Example:
```
output/customer_trusted/part-0000.snappy.parquet
```

To inspect row counts, re-load Parquet files in PySpark:
```python
df = spark.read.parquet("output/customer_trusted")
df.count()
```

---

## ğŸ“š Reference Repositories

- [Official Udacity STEDI Project](https://github.com/udacity/nd027-Data-Engineering-Data-Lakes-AWS-Exercises)


## ğŸ‘¨â€ğŸ’» Author

This project was developed by Rikki Martz as part of the Udacity Data Engineering Nanodegree program.

---

## ğŸ“ License

This project is for educational purposes. No proprietary data is shared.
